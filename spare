docker compose

version: '3.7'

# Reusable configuration block for Spark services
x-spark-common: &spark-common
  # Spark 3.5.7 - known working tag
  image: spark:3.5.7-python3
  volumes:
    - ./jobs:/opt/spark/jobs
    - ./data:/opt/spark/data
  networks:
    - sparking-flow

services:
  # 1. Spark Master Service (from your current setup)
  spark-master:
    <<: *spark-common
    container_name: spark-master
    hostname: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "9090:8080"
      - "7077:7077"

  # 2. Spark Worker Service (from your current setup)
  spark-worker-a:
    <<: *spark-common
    container_name: spark-worker-a
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    ports:
      - "9091:8080"
    environment:
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g

  # --- AIRFLOW COMPONENTS START HERE ---

  # 3. Airflow Metadata Database
  postgres:
    image: postgres:16-alpine # Stable, lightweight version
    container_name: postgres
    environment:
      # These variables link to the Airflow services via airflow.env
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-d", "airflow", "-U", "airflow"]
      timeout: 5s
      retries: 5
    networks:
      - sparking-flow

  # 4. Custom Airflow Webserver (UI)
  webserver:
    # Builds the custom image defined in your Dockerfile (Step 2 below)
    build: .
    image: sparkingflow-image
    container_name: airflow-webserver
    restart: always
    depends_on:
      postgres:
        condition: service_healthy # Wait for the DB to be ready
    ports:
      - "8080:8080"
    volumes:
      # Critical for DAGs and logs
      - ./dags:/opt/airflow/dags:rw
      - ./logs:/opt/airflow/logs:rw
    env_file:
      - airflow.env
    command: webserver
    networks:
      - sparking-flow

  # 5. Custom Airflow Scheduler
  scheduler:
    build: .
    image: sparkingflow-image
    container_name: airflow-scheduler
    restart: always
    depends_on:
      webserver:
        condition: service_started
      postgres:
        condition: service_healthy
    volumes:
      - ./dags:/opt/airflow/dags:rw
      - ./logs:/opt/airflow/logs:rw
    env_file:
      - airflow.env
    # Command: Migrate DB, Create Admin User, then Start Scheduler
    command: bash -c "airflow db migrate && airflow users create --username admin --firstname Yu --lastname Spark --role Admin --email admin@example.com --password admin || true && airflow scheduler"
    networks:
      - sparking-flow

networks:
  sparking-flow:
    driver: bridge


Dockerfile

# -------------------------------------------------------
# Base Image: Official Apache Airflow (Python 3.11)
# -------------------------------------------------------
FROM apache/airflow:2.9.3-python3.11

# -------------------------------------------------------
# Switch to root user to install system dependencies
# -------------------------------------------------------
USER root

# Install OpenJDK 17 (for Spark) and Git
RUN apt-get update && \
    apt-get install -y --no-install-recommends openjdk-17-jdk-headless git && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# -------------------------------------------------------
# Set Java environment variables (without overwriting PATH)
# -------------------------------------------------------
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# -------------------------------------------------------
# Switch back to the airflow user
# -------------------------------------------------------
USER airflow
WORKDIR /opt/airflow

# -------------------------------------------------------
# Make sure both Airflow and pip-installed binaries are available
# -------------------------------------------------------
ENV PATH="/home/airflow/.local/bin:/usr/local/bin:${PATH}"

# -------------------------------------------------------
# Copy and install Python dependencies
# -------------------------------------------------------
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

# -------------------------------------------------------
# Default working directory
# -------------------------------------------------------
WORKDIR /opt/airflow

# -------------------------------------------------------
# Default entrypoint message
# -------------------------------------------------------
CMD ["bash", "-c", "echo 'âœ… Airflow container is ready. Use docker-compose up to start webserver/scheduler.'"]


airflow.env

AIRFLOW_UID=50000
AIRFLOW_GID=0
AIRFLOW_HOME=/opt/airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
AIRFLOW__CORE__LOAD_EXAMPLES=False
# Connects to the PostgreSQL service container
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
# IMPORTANT: Generate a new key with: python -c "import secrets; print(secrets.token_hex(32))"
AIRFLOW__WEBSERVER__SECRET_KEY=33e045cef7e78f4b826f4f18300908673a118793d26215dc90e385de31acf044
AIRFLOW__CORE__FERNET_KEY=xPnVlw1yJUt6SM-ppAvd0TrIBHvJQRsVo1fgvPrv6dc=

req.txt

pyspark==3.5.0
apache-airflow-providers-apache-spark